{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. End-to-End\tMachine Learning\tProject\n",
    "\n",
    "\n",
    "## Working\twith\tReal\tData \n",
    "\n",
    "-> Check the book page no. 82 for real data sources.\n",
    "\n",
    "# Checklist\n",
    "\n",
    "This\tchecklist\tcan\tguide\tyou\tthrough\tyour\tMachine\tLearning\tprojects.\tThere\tare eight\tmain\tsteps:\n",
    "\t\n",
    "1.\tFrame\tthe\tproblem\tand\tlook\tat\tthe\tbig\tpicture. \n",
    "    \n",
    "    1.1 - Frame\tthe\tProblem \n",
    "        \n",
    "        1.1.1 - The\tfirst\tquestion\tto\task\tyour\tboss\tis\twhat\texactly\tis\tthe\tbusiness\tobjective.\n",
    "        How\tdoes\tthe\tcompany\texpect\tto use\tand\tbenefit\tfrom\tthis\tmodel?\n",
    "        \n",
    "        1.1.2 - The\tnext\tquestion\tto\task\tis\twhat\tthe\tcurrent\tsolution\tlooks\tlike\t(if\tany).\n",
    "        \n",
    "        1.1.3 - with the answers to the above questions you\tare\tnow\tready\tto\tstart\tdesigning\tyour\tsystem. First,\tyou\tneed\tto\tframe\tthe\tproblem:\tis\tit\tsupervised,\tunsupervised,\tor Reinforcement\tLearning?\tIs\tit\ta\tclassification\ttask,\ta\tregression\ttask,\tor something\telse?\t\tShould\tyou\tuse\tbatch\tlearning\tor\tonline\tlearning\ttechniques? \n",
    "        \n",
    "    1.2 - Select\ta\tPerformance\tMeasure.\n",
    "    \n",
    "        1.2.1 - Your\tnext\tstep\tis\tto\tselect\ta\tperformance\tmeasure according to your planned system\n",
    "        - for example if your required system is supervised, regression task, A\ttypical\tperformance measure\tfor\tregression\tproblems\tis\tthe\tRoot\tMean\tSquare\tError\t(RMSE).\n",
    "        - NOTE -> When outliers are more we would prefer MAE instead of RMSE\n",
    "        \n",
    "    1.3 - Check\tthe\tAssumptions \n",
    "    \n",
    "        1.3.1 - Lastly,\tit\tis\tgood\tpractice\tto\tlist\tand\tverify\tthe\tassumptions\tthat\twere\tmade\tso\tfar (by\tyou\tor\tothers);\tthis\tcan\tcatch\tserious\tissues\tearly\ton.\n",
    "        \n",
    "2.\tGet\tthe\tdata. \n",
    "\n",
    "    2.1 Create\tthe\tWorkspace\n",
    "    \n",
    "        - NOTE -> If\tyou\twould\tlike\tto\twork\tin\tan\tisolated\tenvironment\t(which\tis\tstrongly\trecommended\tso\tyou\tcan work\ton\tdifferent\tprojects\twithout\thaving\tconflicting\tlibrary\tversions),\tinstall\tvirtualenv\n",
    "    \n",
    "    2.2 Download\tthe\tData \n",
    "    \n",
    "        2.2.1 - Create a function to get the data from source systems to your system.\n",
    "        \n",
    "        2.2.2 - Then you can run the function to fetch the latest data or you\tcan\tset\tup\ta\tscheduled\tjob\tto\tdo\tthat\tautomatically\tat regular\tintervals\n",
    "        \n",
    "        2.2.3 - Write a function to load the data using Pandas\n",
    "        \n",
    "    2.3 Take\ta\tQuick\tLook\tat\tthe\tData\tStructure\n",
    "    \n",
    "        2.3.1 - head(), info(), describe(), value_counts() on catagorical columns if any. etc etc\n",
    "        2.3.2 - Another\tquick\tway\tto\tget\ta\tfeel\tof\tthe\ttype\tof\tdata\tyou\tare\tdealing\twith\tis\tto\tplot a\thistogram\tfor\teach\tnumerical\tattribute.\t\n",
    "        2.3.3 - Observe those histograms carefully for some insights\n",
    "            - for example -> check if any data is scaled and/or\tcapped specially your target data\n",
    "                          -> check for feature scales, so you can perform feature scaling\n",
    "                          -> check for histograms that are\ttail\theavy.\tWe\twill\ttry\ttransforming these\tattributes\tlater\ton\tto\thave\tmore\tbell-shaped\tdistributions. \n",
    "\t\n",
    "    2.4 Create\ta\tTest\tSet \n",
    "    \n",
    "        2.4.1 sklearn.model_selection\timport\ttrain_test_split performs purely\trandom\tsampling\tmethods.\tThis\tis\tgenerally fine\tif\tyour\tdataset\tis\tlarge\tenough\t(especially\trelative\tto\tthe\tnumber\tof attributes\n",
    "                        OR\n",
    "         but\tif\tit\tis\tnot, you\tcan\tuse\tScikit-Learn’s\tStratifiedShuffleSplit\tclass to split and train sets on any important feature you feel like is important\n",
    "\n",
    "    \n",
    "3.\tExplore(Discover and Visualize)\tthe\tdata\tto\tgain\tinsights. \n",
    "\n",
    "    - if the\ttraining\tset\tis\tvery\tlarge,\tyou\tmay\twant\tto\tsample\tan exploration\tset,\tto\tmake\tmanipulations\teasy\tand\tfast \n",
    "                        OR\n",
    "     - create\ta\tcopy\tso\tyou\tcan play\twith\tit\twithout\tharming\tthe\ttraining\tset\n",
    "     \n",
    "     3.1 Visualizing\tGeographical\tData \n",
    "     \n",
    "     - create plots to gain insights, for example scatterplot\n",
    "     - play\taround\twith\tvisualization\tparameters\tto\tmake\tthe\tpatterns stand\tout. for example changing the alpha parameter\n",
    "     \n",
    "     3.2 Look for Correlations \n",
    "        \n",
    "      - watch out for data\tquirks or anything unreasonable and then get rid of those data points.\n",
    "      \n",
    "     3.3 Experimenting\twith\tAttribute\tCombinations \n",
    "     \n",
    "     - You\tidentified\ta\tfew\tdata\tquirks\tthat\tyou\tmay\twant\tto clean\tup\tbefore\tfeeding\tthe\tdata\tto\ta\tMachine\tLearning\talgorithm,\tand\tyou found\tinteresting\tcorrelations\tbetween\tattributes,\tin\tparticular\twith\tthe\ttarget attribute.\tYou\talso\tnoticed\tthat\tsome\tattributes\thave\ta\ttail-heavy\tdistribution,\tso you\tmay\twant\tto\ttransform\tthem\t(e.g.,\tby\tcomputing\ttheir\tlogarithm).\tOf\tcourse, your\tmileage\twill\tvary\tconsiderably\twith\teach\tproject,\tbut\tthe\tgeneral\tideas\tare similar.\n",
    "     \n",
    "             3.3.1 - \ttry\tout\tvarious\tattribute\tcombinations\n",
    "             \n",
    "      - This\tround\tof\texploration\tdoes\tnot\thave\tto\tbe\tabsolutely\tthorough;\tthe\tpoint\tis\tto start\toff\ton\tthe\tright\tfoot\tand\tquickly\tgain\tinsights\tthat\twill\thelp\tyou\tget\ta\tfirst reasonably\tgood\tprototype.\tBut\tthis\tis\tan\titerative\tprocess:\tonce\tyou\tget\ta prototype\tup\tand\trunning,\tyou\tcan\tanalyze\tits\toutput\tto\tgain\tmore\tinsights\tand come\tback\tto\tthis\texploration\tstep. \n",
    "\n",
    "4.\tPrepare\tthe\tdata\tto\tbetter\texpose\tthe\tunderlying\tdata\tpatterns\tto\tMachine Learning\talgorithms. \n",
    "\n",
    "      - create functions to prepare the data\n",
    "      - copy the training set again\n",
    "      - may want separate\tthe\tpredictors\tand\tthe\tlabels\tsince\twe\tdon’t\tnecessarily want\tto\tapply\tthe\tsame\ttransformations\tto\tthe\tpredictors\tand\tthe\ttarget\tvalues \n",
    "      \n",
    "      4.1 Data\tCleaning \n",
    "      \n",
    "          4.1.1 - create function to deal with missing data. You have 3 options to do that\n",
    "              \n",
    "              Get\trid\tof\tthe\tcorresponding\tdistricts. \n",
    "              OR\n",
    "              Get\trid\tof\tthe\twhole\tattribute. \n",
    "              OR\n",
    "              Set\tthe\tvalues\tto\tsome\tvalue\t(zero,\tthe\tmean,\tthe\tmedian,\tetc.). \n",
    "      \n",
    "      4.2 - Handling\tText\tand\tCategorical\tAttributes \n",
    "      \n",
    "          4.2.1 - convert\tthese\ttext\tlabels\tto numbers(or encode\tthe\tcategories\tas\tone-hot\tvectors). if possible\n",
    "          - CHECK -> and use sklearn LabelBinarizer class\n",
    "          \n",
    "      4.3 - Custom\tTransformers \n",
    "      \n",
    "          - you\twill\tneed\tto\twrite your\town transformer\tfor\ttasks\tsuch\tas\tcustom\tcleanup\toperations\tor\tcombining\tspecific attributes.\t\n",
    "          - read page 135\n",
    "          - CHECK code\n",
    "          \n",
    "      4.4 - Feature\tScaling \n",
    "      \n",
    "          - One\tof\tthe\tmost\timportant\ttransformations\tyou\tneed\tto\tapply\tto\tyour\tdata\tis feature\tscaling\n",
    "          - two\tcommon\tways for scaling the features:\t\n",
    "              --minmax\tscaling\t(normalization) - sklearn MinMaxScaler\n",
    "              and/or\t\n",
    "              --standardization. - sklearn \tStandardScaler\n",
    "                  standardization\tis\tmuch\tless\taffected\tby\toutliers. \n",
    "                  \n",
    "      4.5 - Transformation\tPipelines\n",
    "      \n",
    "          - there\tare\tmany\tdata\ttransformation\tsteps\tthat\tneed\tto\tbe executed\tin\tthe\tright\torder.\tFortunately,\tScikit-Learn\tprovides\tthe\tPipeline\tclass to\thelp\twith\tsuch\tsequences\tof\ttransformations.\t\n",
    "\n",
    "\n",
    "5.\tExplore\tmany\tdifferent\tmodels\tand\tshort-list\tthe\tbest\tones. \n",
    "\n",
    "\n",
    "\ttry\tout\tmany\tother\tmodels\tfrom\tvarious\tcategories\tof\tMachine Learning\talgorithms\t(like several\tSupport\tVector\tMachines\twith\tdifferent\tkernels, possibly\ta\tneural\tnetwork,\tetc.),\twithout\tspending\ttoo\tmuch\ttime\ttweaking\tthe hyperparameters.\tThe\tgoal\tis\tto\tshortlist\ta\tfew\t(two\tto\tfive)\tpromising\tmodels.\n",
    "\n",
    "\n",
    "    5.1 Training\tand\tEvaluating\ton\tthe\tTraining\tSet\n",
    "    \n",
    "        5.1.1 - Train a model (like linear regression model)\n",
    "        5.1.2 - Evaluate the model performance(using the Performance measure like RMSE, MAE etc)\n",
    "        5.1.3 - Deduce the info of how is your model performing (like is it Okay, Underfitting or overfitting)\n",
    "            - if its Underfitting, possible things that you can do are\n",
    "                -- select\ta\tmore\tpowerful\tmodel, feed\tthe\ttraining\talgorithm with\tbetter\tfeatures,\tor\tto\treduce\tthe\tconstraints\ton\tthe\tmodel\n",
    "        5.1.4 - but\tfirst more\tcomplex\tmodel\tto\tsee how\tit\tdoes. Try different models and Evaluate the using cross-validation\n",
    "        5.1.5 - repeat the steps till you find a descent model to bet on.\n",
    "                \n",
    "    5.2 Better\tEvaluation\tUsing\tCross-Validation\n",
    "    \n",
    "        - if the score\ton\tthe\ttraining\tset\tis\tmuch higher than\ton\tthe\tvalidation\tsets, meaning\tthat\tthe\tmodel\tis\tstill\toverfitting\tthe\ttraining\tset.\n",
    "        - Possible\tsolutions\tfor overfitting\tare\tto\tsimplify\tthe\tmodel,\tconstrain\tit\t(i.e.,\tregularize\tit),\tor\tget\ta\tlot more\ttraining\tdata.\n",
    "        \n",
    "        \n",
    "-TIP \n",
    "\n",
    "You\tshould\tsave\tevery\tmodel\tyou\texperiment\twith,\tso\tyou\tcan\tcome\tback\teasily\tto\tany\tmodel\tyou\n",
    "want.\tMake\tsure\tyou\tsave\tboth\tthe\thyperparameters\tand\tthe\ttrained\tparameters,\tas\twell\tas\tthe\tcrossvalidation\tscores\tand\tperhaps\tthe\tactual\tpredictions\tas\twell.\tThis\twill\tallow\tyou\tto\teasily\tcompare scores\tacross\tmodel\ttypes,\tand\tcompare\tthe\ttypes\tof\terrors\tthey\tmake.\tYou\tcan\teasily\tsave\tScikitLearn\tmodels\tby\tusing\tPython’s\tpickle\tmodule,\tor\tusing\tsklearn.externals.joblib,\twhich\tis more\tefficient\tat\tserializing\tlarge\tNumPy\tarrays:\n",
    "\n",
    "    \n",
    "6.\tFine-tune\tyour\tmodels\tand\tcombine\tthem\tinto\ta\tgreat\tsolution. \n",
    "\n",
    "    - Let’s\tassume\tthat\tyou\tnow\thave\ta\tshortlist\tof\tpromising\tmodels.\tYou\tnow\tneed to\tfine-tune\tthem. \tfew\tways to fine-tune your model.\n",
    "    \n",
    "    -- Grid\tSearch\n",
    "    \n",
    "        The\tgrid\tsearch\tapproach\tis\tfine\twhen\tyou\tare\texploring\trelatively\tfew combinations,\tlike\tin\tthe\tprevious\texample,\n",
    "    \n",
    "        Use Scikit-Learn’s\tGridSearchCV to tell which\thyperparameters\tyou\twant\tit\tto\texperiment\twith,\tand what\tvalues\tto\ttry\tout, \tit\twill\tevaluate\tall\tthe\tpossible\tcombinations\tof hyperparameter\tvalues,\tusing\tcross-validation and return the best combination of the hyperparameters.\n",
    "        \n",
    "        - TIP \n",
    "        When\tyou\thave\tno\tidea\twhat\tvalue\ta\thyperparameter\tshould\thave,\ta\tsimple\tapproach\tis\tto\ttry\tout consecutive\tpowers\tof\t10\t(or\ta\tsmaller\tnumber\tif\tyou\twant\ta\tmore\tfine-grained\tsearch,\tas\tshown\tin this\texample\twith\tthe\tn_estimators\thyperparameter).\n",
    "\n",
    "    \n",
    "        - TIP \n",
    "        Don’t\tforget\tthat\tyou\tcan\ttreat\tsome\tof\tthe\tdata\tpreparation\tsteps\tas\thyperparameters.\tFor\texample, the\tgrid\tsearch\twill\tautomatically\tfind\tout\twhether\tor\tnot\tto\tadd\ta\tfeature\tyou\twere\tnot\tsure\tabout (e.g.,\tusing\tthe\tadd_bedrooms_per_room\thyperparameter\tof\tyour\tCombinedAttributesAdder transformer).\tIt\tmay\tsimilarly\tbe\tused\tto\tautomatically\tfind\tthe\tbest\tway\tto\thandle\toutliers,\tmissing features,\tfeature\tselection,\tand\tmore.\n",
    "        \n",
    "     -- Randomized\tSearch\n",
    "     \n",
    "         when\tthe\thyperparameter\tsearch space\tis\tlarge,\tit\tis\toften\tpreferable\tto\tuse\tRandomizedSearchCV\tinstead.\n",
    "         \n",
    "     -- Ensemble\tMethods \n",
    "     \n",
    "         try\tto\tcombine\tthe\tmodels\tthat perform\tbest. (see chapter 7)\n",
    "    \n",
    "     -- Analyze\tthe\tBest\tModels\tand\tTheir\tErrors \n",
    "     \n",
    "         You\twill\toften\tgain\tgood\tinsights\ton\tthe\tproblem\tby\tinspecting\tthe\tbest\tmodels. For\texample,\tthe\tRandomForestRegressor\tcan\tindicate\tthe\trelative\timportance of\teach\tattribute\tfor\tmaking\taccurate\tpredictions:\n",
    "                    >>>\tfeature_importances\t=\tgrid_search.best_estimator_.feature_importances_ \n",
    "                    >>>\tfeature_importances\n",
    "                    \n",
    "         --- With\tthis\tinformation,\tyou\tmay\twant\tto\ttry\tdropping\tsome\tof\tthe\tless\tuseful features\n",
    "         --- You\tshould\talso\tlook\tat\tthe\tspecific\terrors\tthat\tyour\tsystem\tmakes,\tthen\ttry\tto understand\twhy\tit\tmakes\tthem\tand\twhat\tcould\tfix\tthe\tproblem\t(adding\textra features\tor,\ton\tthe\tcontrary,\tgetting\trid\tof\tuninformative\tones,\tcleaning\tup outliers,\tetc.)\n",
    "         \n",
    "      -- Evaluate\tYour\tSystem\ton\tthe\tTest\tSet\n",
    "      \n",
    "          After\ttweaking\tyour\tmodels\tfor\ta\twhile,\tyou\teventually\thave\ta\tsystem\tthat performs\tsufficiently\twell.\tNow\tis\tthe\ttime\tto\tevaluate\tthe\tfinal\tmodel\ton\tthe\ttest set.\n",
    "    \n",
    "7.\tPresent\tyour\tsolution. \n",
    "\n",
    "    - Now\tcomes\tthe\tproject\tprelaunch\tphase:\tyou\tneed\tto\tpresent\tyour\tsolution (highlighting\twhat\tyou\thave\tlearned,\twhat\tworked\tand\twhat\tdid\tnot,\twhat assumptions\twere\tmade,\tand\twhat\tyour\tsystem’s\tlimitations\tare),\tdocument everything,\tand\tcreate\tnice\tpresentations\twith\tclear\tvisualizations\tand\teasy-toremember\tstatements\t(e.g.,\t“the\tmedian\tincome\tis\tthe\tnumber\tone\tpredictor\tof housing\tprices”).\n",
    "    \n",
    "8.\tLaunch,\tmonitor,\tand\tmaintain\tyour\tsystem. \n",
    "\n",
    "    - You\tneed\tto\tget\tyour\tsolution\tready\tfor production,\tin\tparticular\tby\tplugging\tthe\tproduction\tinput\tdata\tsources\tinto\tyour system\tand\twriting\ttests. \n",
    "    \n",
    "    - You\talso\tneed\tto\twrite\tmonitoring\tcode\tto\tcheck\tyour\tsystem’s\tlive\tperformance at\tregular\tintervals\tand\ttrigger\talerts\twhen\tit\tdrops.\t\n",
    "    \n",
    "    - Evaluating\tyour\tsystem’s\tperformance\twill\trequire\tsampling\tthe\tsystem’s predictions\tand\tevaluating\tthem.(done by analysts\tmay\tbe\tfield\texperts)\n",
    "    \n",
    "    - You\tshould\talso\tmake\tsure\tyou\tevaluate\tthe\tsystem’s\tinput\tdata\tquality. \n",
    "    \n",
    "    - Finally,\tyou\twill\tgenerally\twant\tto\ttrain\tyour\tmodels\ton\ta\tregular\tbasis\tusing fresh\tdata.\tYou\tshould\tautomate\tthis\tprocess\tas\tmuch\tas\tpossible\n",
    "    \n",
    "    - If\tyour\tsystem\tis an\tonline\tlearning\tsystem,\tyou\tshould\tmake\tsure\tyou\tsave\tsnapshots\tof\tits\tstate at\tregular\tintervals\tso\tyou\tcan\teasily\troll\tback\tto\ta\tpreviously\tworking\tstate.\n",
    "    \n",
    "    \n",
    "As\tyou\tcan\tsee,\tmuch\tof\tthe\twork\tis\tin\tthe\tdata\tpreparation\tstep,\tbuilding monitoring\ttools,\tsetting\tup\thuman\tevaluation\tpipelines,\tand\tautomating\tregular model\ttraining\n",
    "\n",
    "\n",
    "Obviously,\tyou\tshould\tfeel\tfree\tto\tadapt\tthis\tchecklist\tto\tyour\tneeds.\n",
    "\n",
    "\n",
    "\n",
    "# 3. Classification\n",
    "\n",
    "## MNIST\n",
    "\n",
    "- loading from sklearn\n",
    "- viewing a data instance\n",
    "\n",
    "## Training\ta\tBinary\tClassifier\n",
    "\n",
    "- create\tthe\ttarget\tvectors\tfor\tthis\tclassification\ttask\n",
    "- pick\ta\tclassifier\tand\ttrain\tit\n",
    "    -- here we start with -> Stochastic\tGradient\tDescent\t(SGD)\tclassifier\n",
    "                          -> SGD\tdeals\twith training\tinstances\tindependently,\tone\tat\ta\ttime\t(which\talso\tmakes\tSGD\twell suited\tfor\tonline\tlearning) and hence is \tcapable\tof handling\tvery\tlarge\tdatasets\tefficiently.\n",
    "                          \n",
    "## Performance\tMeasures \n",
    "\n",
    "\n",
    "- TIPS \n",
    "Occasionally\tyou\twill\tneed\tmore\tcontrol\tover\tthe\tcross-validation\tprocess\tthan\twhat\tScikit-Learn provides\toff-the-shelf.\tIn\tthese\tcases,\tyou\tcan\timplement\tcross-validation\tyourself\n",
    "\n",
    "CHECK code for implementation\n",
    "\n",
    "The\tStratifiedKFold\tclass\tperforms\tstratified\tsampling\t(as\texplained\tin\tChapter\t2)\tto\tproduce\tfolds that\tcontain\ta\trepresentative\tratio\tof\teach\tclass\n",
    "\n",
    "### Measuring\tAccuracy\tUsing\tCross-Validation (Never used, Not Recommeded)\n",
    "\n",
    "- accuracy\tis\tgenerally\tnot\tthe\tpreferred\tperformance measure\tfor\tclassifiers,\tespecially\twhen\tyou\tare\tdealing\twith\tskewed\tdatasets (i.e.,\twhen\tsome\tclasses\tare\tmuch\tmore\tfrequent\tthan\tothers).\n",
    "\n",
    "### Confusion\tMatrix \n",
    "\n",
    "1) you\tcan\tuse\tthe\tcross_val_predict()\tfunction that returns the predictions\tmade\ton\teach\ttest\tfold. then\n",
    "2) get\tthe\tconfusion\tmatrix\tusing\tthe\tconfusion_matrix() function.\n",
    "\n",
    "- \tthe\taccuracy\tof\tthe positive\tpredictions;\tthis\tis\tcalled\tthe\tprecision\t\n",
    "- \trecall,\talso\tcalled\tsensitivity\tor true\tpositive\trate\t(TPR)\n",
    "\n",
    "### Precision\tand\tRecall \n",
    "\n",
    "- CHECK code ->\tfrom\tsklearn.metrics\timport\tprecision_score,\trecall_score\n",
    "\n",
    "{When\tit\tclaims\tan\timage\trepresents\ta\t5,\tit\tis\tcorrect\tonly\t77%(estimated seeing precision score)\tof\tthe time.\tMoreover,\tit\tonly\tdetects\t80%(estimated seeing recall score)\tof\tthe\t5s}\n",
    "\n",
    "- the\tclassifier\twill\tonly\tget\ta\thigh F1\tscore\tif\tboth\trecall\tand\tprecision\tare\thigh. \n",
    "- CHECK code -> from\tsklearn.metrics\timport\tf1_score \n",
    "\n",
    "The\tF1\tscore\tfavors\tclassifiers\tthat\thave\tsimilar\tprecision\tand\trecall.\tThis\tis\tnot always\twhat\tyou\twant:\tin\tsome\tcontexts\tyou\tmostly\tcare\tabout\tprecision,\tand\tin other\tcontexts\tyou\treally\tcare\tabout\trecall.\n",
    "\n",
    "### Precision/Recall\tTradeoff \n",
    "\n",
    "- SGD Classifier computes\ta\tscore\tbased\ton\ta decision\tfunction,\tand\tif\tthat\tscore\tis\tgreater\tthan\ta\tthreshold,\tit\tassigns\tthe instance\tto\tthe\tpositive\tclass,\tor\telse\tit\tassigns\tit\tto\tthe\tnegative\tclass.\n",
    "\n",
    "- Scikit-Learn\tdoes\tnot\tlet\tyou\tset\tthe\tthreshold\tdirectly,\tbut\tit\tdoes\tgive\tyou access\tto\tthe\tdecision\tscores\tthat\tit\tuses\tto\tmake\tpredictions.\tInstead\tof\tcalling the\tclassifier’s\tpredict()\tmethod,\tyou\tcan\tcall\tits\tdecision_function() method,\twhich\treturns\ta\tscore\tfor\teach\tinstance,\tand\tthen\tmake\tpredictions based\ton\tthose\tscores\tusing\tany\tthreshold you want to\n",
    "\n",
    "- CHECK code -> \n",
    "== READ THIS PART AGAIN\n",
    "\n",
    "-TIP\n",
    "If\tsomeone\tsays\t“let’s\treach\t99%\tprecision,”\tyou\tshould\task,\t“at\twhat\trecall?”\n",
    "\n",
    "### The\tROC\tCurve\n",
    "\n",
    "- The\treceiver\toperating\tcharacteristic\t(ROC)\tcurve\tis\tanother\tcommon\ttool\tused with\tbinary\tclassifiers.\n",
    "- area\tunder\tthe\tcurve\t(AUC).\n",
    "\n",
    "== READ THIS PART AGAIN\n",
    "\n",
    "-TIP\n",
    "\n",
    "- Since\tthe\tROC\tcurve\tis\tso\tsimilar\tto\tthe\tprecision/recall\t(or\tPR)\tcurve,\tyou\tmay\twonder\thow\tto decide\twhich\tone\tto\tuse.\tAs\ta\trule\tof\tthumb,\tyou\tshould\tprefer\tthe\tPR\tcurve\twhenever\tthe\tpositive class\tis\trare\tor\twhen\tyou\tcare\tmore\tabout\tthe\tfalse\tpositives\tthan\tthe\tfalse\tnegatives,\tand\tthe\tROC curve\totherwise.\t\n",
    "\n",
    "\n",
    "Hopefully\tyou\tnow\tknow\thow\tto\ttrain\tbinary\tclassifiers,\tchoose\tthe\tappropriate metric\tfor\tyour\ttask,\tevaluate\tyour\tclassifiers\tusing\tcross-validation,\tselect\tthe precision/recall\ttradeoff\tthat\tfits\tyour needs,\tand\tcompare\tvarious\tmodels\tusing ROC\tcurves\tand\tROC\tAUC\tscores.\t\n",
    "\n",
    "\n",
    "## Multiclass\tClassification \n",
    "\n",
    "- \tmulticlass\tclassifiers (also\tcalled\tmultinomial\tclassifiers)\tcan\tdistinguish\tbetween\tmore\tthan\ttwo classes.\n",
    "\n",
    "- Some\talgorithms\t(such\tas\tRandom\tForest\tclassifiers\tor\tnaive\tBayes\tclassifiers) are\tcapable\tof\thandling\tmultiple\tclasses\tdirectly.\tOthers\t(such\tas\tSupport\tVector Machine\tclassifiers\tor\tLinear\tclassifiers)\tare\tstrictly\tbinary\tclassifiers.\tHowever, there\tare\tvarious\tstrategies\tthat\tyou\tcan\tuse\tto\tperform\tmulticlass\tclassification using\tmultiple\tbinary\tclassifiers.\n",
    "    -- strategies -> \toneversus-all\t(OvA)\tstrategy\t(also\tcalled\tone-versus-the-rest). \n",
    "                  ->\tone-versus-one\t(OvO)\tstrategy\n",
    "                        The\tmain\tadvantage\tof\tOvO\tis\tthat\teach\tclassifier\tonly\tneeds\tto\tbe\ttrained\ton the\tpart\tof\tthe\ttraining\tset\tfor\tthe\ttwo\tclasses\tthat\tit\tmust\tdistinguish. \n",
    "                        \n",
    "- Some\talgorithms\t(such\tas\tSupport\tVector\tMachine\tclassifiers)\tscale\tpoorly\twith the\tsize\tof\tthe\ttraining\tset,\tso\tfor\tthese\talgorithms\tOvO\tis\tpreferred\tsince\tit\tis faster\tto\ttrain\tmany\tclassifiers\ton\tsmall\ttraining\tsets\tthan\ttraining\tfew\tclassifiers on\tlarge\ttraining\tsets.\tFor\tmost\tbinary\tclassification\talgorithms,\thowever,\tOvA\tis preferred\n",
    "\n",
    "- Scikit-Learn\tdetects\twhen\tyou\ttry\tto\tuse\ta\tbinary\tclassification\talgorithm\tfor\ta multiclass\tclassification\ttask,\tand\tit\tautomatically\truns\tOvA\t(except\tfor\tSVM classifiers\tfor\twhich\tit\tuses\tOvO).\n",
    "\n",
    "- Now\tof\tcourse\tyou\twant\tto\tevaluate\tthese\tclassifiers.\tAs\tusual,\tyou\twant\tto\tuse cross-validation.\tLet’s\tevaluate\tthe\tSGDClassifier’s\taccuracy\tusing\tthe cross_val_score()\tfunction:\n",
    "\n",
    "\n",
    "## Error\tAnalysis \n",
    "\n",
    "- \tif\tthis\twere\ta\treal\tproject,\tyou\twould\tfollow\tthe\tsteps\tin\tyour Machine\tLearning\tproject\tchecklist\t(see\tAppendix\tB):\texploring\tdata preparation\toptions,\ttrying\tout\tmultiple\tmodels,\tshortlisting\tthe\tbest\tones\tand fine-tuning\ttheir\thyperparameters\tusing\tGridSearchCV,\tand\tautomating\tas\tmuch as\tpossible,\tas\tyou\tdid\tin\tthe\tprevious\tchapter.\tHere,\twe\twill\tassume\tthat\tyou have\tfound\ta\tpromising\tmodel\tand\tyou\twant\tto\tfind\tways\tto\timprove\tit.\tOne way\tto\tdo\tthis\tis\tto\tanalyze\tthe\ttypes\tof\terrors\tit\tmakes. \n",
    "\n",
    "- First,\tyou\tcan\tlook\tat\tthe\tconfusion\tmatrix.\tYou\tneed\tto\tmake\tpredictions\tusing the\tcross_val_predict()\tfunction,\tthen\tcall\tthe\tconfusion_matrix()\tfunction, just\tlike\tyou\tdid\tearlier:\n",
    "\n",
    "- It’s\toften\tmore\tconvenient\tto\tlook\tat\tan\timage representation\tof\tthe\tconfusion\tmatrix(like heatmap)\n",
    "\n",
    "- Analyzing\tthe\tconfusion\tmatrix\tcan\toften\tgive\tyou\tinsights\ton\tways\tto\timprove your\tclassifier.\t\n",
    "\n",
    "- Analyzing\tindividual\terrors\tcan\talso\tbe\ta\tgood\tway\tto\tgain\tinsights\ton\twhat\tyour classifier\tis\tdoing\tand\twhy\tit\tis\tfailing,\tbut\tit\tis\tmore\tdifficult\tand\ttime consuming.\n",
    "\n",
    "- READ THIS SECTION WHEN EVER YOU WANT TO IMPROVE YOUR MODEL AND TO GAIN SOME IDEAS WHAT ACTUALLY YOU CAN DO.\n",
    "\n",
    "## Multilabel\tClassification \n",
    "\n",
    "- In\tsome cases\tyou\tmay\twant\tyour\tclassifier\tto\toutput\tmultiple\tclasses\tfor\teach\tinstance. \n",
    "\n",
    "- CHECK code on how to do that\n",
    "\n",
    "- choose a multilabel\tclassification model like KNeighborsClassifier\n",
    "\n",
    "- There\tare\tmany\tways\tto\tevaluate\ta\tmultilabel\tclassifier,\tand\tselecting\tthe\tright metric\treally\tdepends\ton\tyour\tproject.\tFor\texample,\tone\tapproach\tis\tto\tmeasure the\tF1\tscore\tfor\teach\tindividual\tlabel\t(or\tany\tother\tbinary\tclassifier\tmetric discussed\tearlier),\tthen\tsimply\tcompute\tthe\taverage\tscore.\t\n",
    "\n",
    "## Multioutput\tClassification \n",
    "\n",
    "- multioutput-multiclass\tclassification\t(or\tsimply\tmultioutput\tclassification).\tIt\tis simply\ta\tgeneralization\tof\tmultilabel\tclassification\twhere\teach\tlabel\tcan\tbe multiclass\t(i.e.,\tit\tcan\thave\tmore\tthan\ttwo\tpossible\tvalues). \n",
    "\n",
    "- example -> let’s\tbuild\ta\tsystem\tthat\tremoves\tnoise\tfrom\timages.\tIt\twill take\tas\tinput\ta\tnoisy\tdigit\timage,\tand\tit\twill\t(hopefully)\toutput\ta\tclean\tdigit image,\trepresented\tas\tan\tarray\tof\tpixel\tintensities,\tjust\tlike\tthe\tMNIST\timages. \n",
    "\n",
    "- Notice\tthat\tthe\tclassifier’s\toutput\tis\tmultilabel\t(one\tlabel\tper\tpixel)\tand\teach label\tcan\thave\tmultiple\tvalues\t(pixel\tintensity\tranges\tfrom\t0\tto\t255).\tIt\tis\tthus\tan example\tof\ta\tmultioutput\tclassification\tsystem\n",
    "\n",
    "- This\ttechnique\tof\tartificially\tgrowing\tthe\ttraining\tset\tis\tcalled **data\taugmentation\tor\ttraining\tset\texpansion**\n",
    "\n",
    "\n",
    "# 4.\tTraining\tModels\n",
    "\n",
    "- Indeed,\tin\tmany\tsituations\tyou\tdon’t\treally\tneed\tto\tknow\tthe\timplementation details. However,\thaving\ta\tgood\tunderstanding\tof\thow\tthings\twork\tcan\thelp\tyou\tquickly home\tin\ton\tthe\tappropriate\tmodel,\tthe\tright\ttraining\talgorithm\tto\tuse,\tand\ta\tgood set\tof\thyperparameters\tfor\tyour\ttask. \tUnderstanding\twhat’s\tunder\tthe\thood\twill also\thelp\tyou\tdebug\tissues\tand\tperform\terror\tanalysis\tmore\tefficiently\n",
    "\n",
    "## Linear\tRegression \n",
    "\n",
    "### The\tNormal\tEquation\n",
    "\n",
    "### Computational\tComplexity \n",
    "\n",
    "- WARNING \n",
    "The\tNormal\tEquation\tgets\tvery\tslow\twhen\tthe\tnumber\tof\tfeatures\tgrows\tlarge\t(e.g.,\t100,000).\n",
    "\n",
    "{\n",
    "    you can learn to compute the computation time from the below statement \n",
    "\n",
    "\tO(n2.4)\tto\tO(n3)\t(depending\ton\tthe implementation).\tIn\tother\twords,\tif\tyou\tdouble\tthe\tnumber\tof\tfeatures,\tyou multiply\tthe\tcomputation\ttime\tby\troughly\t2 to the power of 2.4\t=\t5.3\tto\t2 to the power of 3\t=\t8.\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "- Now\twe\twill\tlook\tat\tvery\tdifferent\tways\tto\ttrain\ta\tLinear\tRegression\tmodel, better\tsuited\tfor\tcases\twhere\tthere\tare\ta\tlarge\tnumber\tof\tfeatures,\tor\ttoo\tmany training\tinstances\tto\tfit\tin\tmemory.\n",
    "\n",
    "### Gradient\tDescent \n",
    "\n",
    "- Gradient\tDescent\tis\ta\tvery\tgeneric\toptimization\talgorithm\tcapable\tof\tfinding optimal\tsolutions\tto\ta\twide\trange\tof\tproblems.\tThe\tgeneral\tidea\tof\tGradient Descent\tis\tto\ttweak\tparameters\titeratively\tin\torder\tto\tminimize\ta\tcost\tfunction.\n",
    "\n",
    "- you\tstart\tby\tfilling\tθ\twith\trandom\tvalues\t(this\tis\tcalled\trandom initialization)\n",
    "\n",
    "- An\timportant\tparameter\tin\tGradient\tDescent\tis\tthe\tsize\tof\tthe\tsteps,\tdetermined by\tthe\tlearning\trate\thyperparameter\n",
    "\n",
    "- Finally,\tnot\tall\tcost\tfunctions\tlook\tlike\tnice\tregular\tbowls.\tThere\tmay\tbe\tholes, ridges,\tplateaus,\tand\tall\tsorts\tof\tirregular\tterrains,\tmaking\tconvergence\tto\tthe minimum\tvery\tdifficult.\n",
    "\n",
    "- \tthe\tMSE\tcost\tfunction\tfor\ta\tLinear\tRegression\tmodel\thappens\tto\tbe a\tconvex\tfunction. It\tis\talso\ta\tcontinuous\tfunction\twith\ta slope\tthat\tnever\tchanges\tabruptly.\n",
    "\n",
    "- **WARNING** \n",
    "- When\tusing\tGradient\tDescent,\tyou\tshould\tensure\tthat\tall\tfeatures\thave\ta\tsimilar\tscale\t(e.g.,\tusing Scikit-Learn’s\tStandardScaler\tclass),\tor\telse\tit\twill\ttake\tmuch\tlonger\tto\tconverge.\n",
    "\n",
    "#### Batch\tGradient\tDescent \n",
    "    \n",
    "- **WARNING**\n",
    "- \tterribly\tslow\ton\tvery\tlarge\ttraining\tsets\t(but\twe\twill\tsee\tmuch faster\tGradient\tDescent\talgorithms\tshortly).\tHowever,\tGradient\tDescent\tscales\twell\twith\tthe\tnumber of\tfeatures;\ttraining\ta\tLinear\tRegression\tmodel\twhen\tthere\tare\thundreds\tof\tthousands\tof\tfeatures\tis much\tfaster\tusing\tGradient\tDescent\tthan\tusing\tthe\tNormal\tEquation\n",
    "\n",
    "- To\tfind\ta\tgood\tlearning\trate,\tyou\tcan\tuse\tgrid\tsearch.\tHowever, you\tmay\twant\tto\tlimit\tthe\tnumber\tof\titerations\tso\tthat\tgrid\tsearch\tcan\teliminate models\tthat\ttake\ttoo\tlong\tto\tconverge. \n",
    "\n",
    "\n",
    "- how to determine the number of iterations, A\tsimple\tsolution\tis\tto\tset\ta\tvery\tlarge\tnumber\tof\titerations\tbut\tto interrupt\tthe\talgorithm\twhen\tthe\tgradient\tvector\tbecomes\ttiny\t—\tthat\tis,\twhen its\tnorm\tbecomes\tsmaller\tthan\ta\ttiny\tnumber\tϵ\t(called\tthe\ttolerance)\t—\tbecause this\thappens\twhen\tGradient\tDescent\thas\t(almost)\treached\tthe\tminimum.\n",
    "\n",
    "#### Stochastic\tGradient\tDescent\n",
    "\n",
    "- When\tthe\tcost\tfunction\tis\tvery\tirregular\t(as\tin\tFigure\t4-6),\tthis\tcan\tactually\thelp the\talgorithm\tjump\tout\tof\tlocal\tminima,\tso\tStochastic\tGradient\tDescent\thas\ta better\tchance\tof\tfinding\tthe\tglobal\tminimum\tthan\tBatch\tGradient\tDescent\tdoes. \n",
    "\n",
    "- Therefore\trandomness\tis\tgood\tto\tescape\tfrom\tlocal\toptima,\tbut\tbad\tbecause\tit means\tthat\tthe\talgorithm\tcan\tnever\tsettle\tat\tthe\tminimum.\tOne\tsolution\tto\tthis dilemma\tis\tto\tgradually\treduce\tthe\tlearning\trate. This\tprocess\tis called\tsimulated\tannealing. The\tfunction\tthat determines\tthe\tlearning\trate\tat\teach\titeration\tis\tcalled\tthe\tlearning\tschedule.\n",
    "\n",
    "- To\tperform\tLinear\tRegression\tusing\tSGD\twith\tScikit-Learn,\tyou\tcan\tuse\tthe SGDRegressor\tclass,\twhich\tdefaults\tto\toptimizing\tthe\tsquared\terror\tcost function.\t\n",
    "\n",
    "- CHECK code\n",
    "\n",
    "#### Mini-batch\tGradient\tDescent \n",
    "\n",
    "- \tThe\tmain\tadvantage\tof\tMini-batch\tGD\tover Stochastic\tGD\tis\tthat\tyou\tcan\tget\ta\tperformance\tboost\tfrom\thardware optimization\tof\tmatrix\toperations,\tespecially\twhen\tusing\tGPUs. \n",
    "\n",
    "\n",
    "Batch\tGD’s\tpath actually\tstops\tat\tthe\tminimum,\twhile\tboth\tStochastic\tGD\tand\tMini-batch\tGD continue\tto\twalk\taround.\tHowever,\tdon’t\tforget\tthat\tBatch\tGD\ttakes\ta\tlot\tof\ttime to\ttake\teach\tstep,\tand\tStochastic\tGD\tand\tMini-batch\tGD\twould\talso\treach\tthe minimum\tif\tyou\tused\ta\tgood\tlearning\tschedule.\n",
    "\n",
    "- NOTE \n",
    "- There\tis\talmost\tno\tdifference\tafter\ttraining:\tall\tthese\talgorithms\tend\tup\twith\tvery\tsimilar\tmodels\tand make\tpredictions\tin\texactly\tthe\tsame\tway.\n",
    "\n",
    "\n",
    "## Polynomial\tRegression\n",
    "\n",
    "- What\tif\tyour\tdata\tis\tactually\tmore\tcomplex\tthan\ta\tsimple\tstraight\tline? Surprisingly,\tyou\tcan\tactually\tuse\ta\tlinear\tmodel\tto\tfit\tnonlinear\tdata.\tA\tsimple way\tto\tdo\tthis\tis\tto\tadd\tpowers\tof\teach\tfeature\tas\tnew\tfeatures,\tthen\ttrain\ta linear\tmodel\ton\tthis\textended\tset\tof\tfeatures.\tThis\ttechnique\tis\tcalled Polynomial\tRegression. \n",
    "\n",
    "- \tuse\tScikit-Learn’s PolynomialFeatures\tclass\tto\ttransform\tour\ttraining\tdata,\tadding\tthe\tsquare (2nd-degree\tpolynomial)\tof\teach\tfeature\tin\tthe\ttraining\tset\tas\tnew\tfeatures\n",
    "- CHECK code\n",
    "\n",
    "- Note\tthat\twhen\tthere\tare\tmultiple\tfeatures,\tPolynomial\tRegression\tis\tcapable\tof finding\trelationships\tbetween\tfeatures\t(which\tis\tsomething\ta\tplain\tLinear Regression\tmodel\tcannot\tdo).\t\n",
    "\n",
    "## Learning\tCurves \n",
    "\n",
    "- how\tcan\tyou\tdecide\thow\tcomplex\tyour\tmodel should\tbe?\tHow\tcan\tyou\ttell\tthat\tyour\tmodel\tis\toverfitting\tor\tunderfitting\tthe data? \n",
    "\n",
    "- \tused\tcross-validation\tto\tget\tan\testimate\tof\ta\tmodel’s generalization\tperformance.\tIf\ta\tmodel\tperforms\twell\ton\tthe\ttraining\tdata\tbut generalizes\tpoorly\taccording\tto\tthe\tcross-validation\tmetrics,\tthen\tyour\tmodel\tis overfitting.\tIf\tit\tperforms\tpoorly\ton\tboth,\tthen\tit\tis\tunderfitting.\tThis\tis\tone\tway to\ttell\twhen\ta\tmodel\tis\ttoo\tsimple\tor\ttoo\tcomplex. \n",
    "\n",
    "- Another\tway\tis\tto\tlook\tat\tthe\tlearning\tcurves\n",
    "- CHECk code\n",
    "\n",
    "-> These\tlearning\tcurves\tare\ttypical\tof\tan\tunderfitting\tmodel.\tBoth\tcurves\thave reached\ta\tplateau;\tthey\tare\tclose\tand\tfairly\thigh.\n",
    "\n",
    "- TIP \n",
    "- If\tyour\tmodel\tis\tunderfitting\tthe\ttraining\tdata,\tadding\tmore\ttraining\texamples\twill\tnot\thelp.\tYou\tneed to\tuse\ta\tmore\tcomplex\tmodel\tor\tcome\tup\twith\tbetter\tfeatures.\n",
    "\n",
    "-> These\tlearning\tcurves\tlook\ta\tbit\tlike\tthe\tprevious\tones,\tbut\tthere\tare\ttwo\tvery important\tdifferences:\n",
    "\t\n",
    "- The\terror\ton\tthe\ttraining\tdata\tis\tmuch\tlower\tthan\twith\tthe\tLinear Regression\tmodel.\n",
    "\n",
    "- There\tis\ta\tgap\tbetween\tthe\tcurves.\tThis\tmeans\tthat\tthe\tmodel\tperforms significantly\tbetter\ton\tthe\ttraining\tdata\tthan\ton\tthe\tvalidation\tdata,\twhich\tis the\thallmark\tof\tan\toverfitting\tmodel.\tHowever,\tif\tyou\tused\ta\tmuch\tlarger training\tset,\tthe\ttwo\tcurves\twould\tcontinue\tto\tget\tcloser.\n",
    "\n",
    "- TIP \n",
    "- One\tway\tto\timprove\tan\toverfitting\tmodel\tis\tto\tfeed\tit\tmore\ttraining\tdata\tuntil\tthe\tvalidation\terror reaches\tthe\ttraining\terror.\n",
    "\n",
    "## Regularized\tLinear\tModels \n",
    "\n",
    "- a\tsimple\tway\tto\tregularize\ta polynomial\tmodel\tis\tto\treduce\tthe\tnumber\tof\tpolynomial\tdegrees\n",
    "\n",
    "- For\ta\tlinear\tmodel,\tregularization\tis\ttypically\tachieved\tby\tconstraining\tthe weights\tof\tthe\tmodel. Ridge\tRegression,\tLasso\tRegression, and\tElastic\tNet,\twhich\timplement\tthree\tdifferent\tways\tto\tconstrain\tthe\tweights.\n",
    "\n",
    "### Ridge\tRegression(\tTikhonov\tregularization)\n",
    "\n",
    "- The\thyperparameter\tα\tcontrols\thow\tmuch\tyou\twant\tto\tregularize\tthe\tmodel.\n",
    "\n",
    "- Note\tthat\tthe\tbias\tterm\tθ0\tis\tnot\tregularized.\n",
    "\n",
    "- the\tregularization\tterm is\tsimply\tequal\tto\t½(∥\tw\t∥2)2,\twhere\t∥\t·\t∥2\trepresents\tthe\tℓ2\tnorm\tof\tthe\tweight vector.\n",
    "\n",
    "- WARNING \n",
    "- It\tis\timportant\tto\tscale\tthe\tdata\t(e.g.,\tusing\ta\tStandardScaler)\tbefore\tperforming\tRidge\tRegression, as\tit\tis\tsensitive\tto\tthe\tscale\tof\tthe\tinput\tfeatures.\tThis\tis\ttrue\tof\tmost\tregularized\tmodels.\n",
    "\n",
    "- Here\tis\thow\tto\tperform\tRidge\tRegression\twith\tScikit-Learn\tusing\ta\tclosed-form\n",
    "solution\t(a\tvariant\tof\tEquation\t4-9\tusing\ta\tmatrix\tfactorization\ttechnique\tby André-Louis\tCholesky):\n",
    ">>>\tfrom\tsklearn.linear_model\timport\tRidge \n",
    ">>>\tridge_reg\t=\tRidge(alpha=1,\tsolver=\"cholesky\") \n",
    ">>>\tridge_reg.fit(X,\ty) \n",
    ">>>\tridge_reg.predict([[1.5]]) array([[\t1.55071465]]) \n",
    "\n",
    "And\tusing\tStochastic\tGradient\tDescent\n",
    ">>>\tsgd_reg\t=\tSGDRegressor(penalty=\"l2\") \n",
    ">>>\tsgd_reg.fit(X,\ty.ravel()) \n",
    ">>>\tsgd_reg.predict([[1.5]]) array([\t1.13500145]) \n",
    "\n",
    "- The\tpenalty\thyperparameter\tsets\tthe\ttype\tof\tregularization\tterm\tto\tuse. Specifying\t\"l2\"\tindicates\tthat\tyou\twant\tSGD\tto\tadd\ta\tregularization\tterm\tto\tthe cost\tfunction\tequal\tto\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm\tof\tthe\tweight\tvector:\tthis\tis simply\tRidge\tRegression.\n",
    "\n",
    "\n",
    "### Lasso\tRegression \n",
    "\n",
    "- Least\tAbsolute\tShrinkage\tand\tSelection\tOperator\tRegression\t(simply\tcalled Lasso\tRegression)\tis\tanother\tregularized\tversion\tof\tLinear\tRegression:\tjust\tlike Ridge\tRegression,\tit\tadds\ta\tregularization\tterm\tto\tthe\tcost\tfunction,\tbut\tit\tuses the\tℓ1\tnorm\tof\tthe\tweight\tvector\tinstead\tof\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm\n",
    "\n",
    "- An\timportant\tcharacteristic\tof\tLasso\tRegression\tis\tthat\tit\ttends\tto\tcompletely eliminate\tthe\tweights\tof\tthe\tleast\timportant\tfeatures\t(i.e.,\tset\tthem\tto\tzero).\n",
    "\n",
    "- \tLasso\tRegression\tautomatically\tperforms feature\tselection\tand\toutputs\ta\tsparse\tmodel\t(i.e.,\twith\tfew\tnonzero\tfeature weights).\n",
    "\n",
    "- CHECK code\n",
    "\n",
    "### Elastic\tNet \n",
    "\n",
    "- Elastic\tNet\tis\ta\tmiddle\tground\tbetween\tRidge\tRegression\tand\tLasso\tRegression. The\tregularization\tterm\tis\ta\tsimple\tmix\tof\tboth\tRidge\tand\tLasso’s\tregularization terms,\tand\tyou\tcan\tcontrol\tthe\tmix\tratio\tr.\tWhen\tr\t=\t0,\tElastic\tNet\tis\tequivalent to\tRidge\tRegression,\tand\twhen\tr\t=\t1,\tit\tis\tequivalent\tto\tLasso\tRegression\n",
    "\n",
    "- CHECK code\n",
    "\n",
    "\n",
    "So\twhen\tshould\tyou\tuse\tplain\tLinear\tRegression\t(i.e.,\twithout\tany regularization),\tRidge,\tLasso,\tor\tElastic\tNet?\tIt\tis\talmost\talways\tpreferable\tto have\tat\tleast\ta\tlittle\tbit\tof\tregularization,\tso\tgenerally\tyou\tshould\tavoid\tplain Linear\tRegression.\tRidge\tis\ta\tgood\tdefault,\tbut\tif\tyou\tsuspect\tthat\tonly\ta\tfew features\tare\tactually\tuseful,\tyou\tshould\tprefer\tLasso\tor\tElastic\tNet\tsince\tthey tend\tto\treduce\tthe\tuseless\tfeatures’\tweights\tdown\tto\tzero\tas\twe\thave\tdiscussed. In\tgeneral,\tElastic\tNet\tis\tpreferred\tover\tLasso\tsince\tLasso\tmay\tbehave erratically\twhen\tthe\tnumber\tof\tfeatures\tis\tgreater\tthan\tthe\tnumber\tof\ttraining instances\tor\twhen\tseveral\tfeatures\tare\tstrongly\tcorrelated. \n",
    "\n",
    "\n",
    "### Early\tStopping \n",
    "\n",
    "- A\tvery\tdifferent\tway\tto\tregularize\titerative\tlearning\talgorithms\tsuch\tas\tGradient Descent\tis\tto\tstop\ttraining\tas\tsoon\tas\tthe\tvalidation\terror\treaches\ta\tminimum. This\tis\tcalled\tearly\tstopping.\tFigure\t4-20\tshows\ta\tcomplex\tmodel\t(in\tthis\tcase\ta high-degree\tPolynomial\tRegression\tmodel)\tbeing\ttrained\tusing\tBatch\tGradient Descent.\tAs\tthe\tepochs\tgo\tby,\tthe\talgorithm\tlearns\tand\tits\tprediction\terror (RMSE)\ton\tthe\ttraining\tset\tnaturally\tgoes\tdown,\tand\tso\tdoes\tits\tprediction\terror on\tthe\tvalidation\tset.\tHowever,\tafter\ta\twhile\tthe\tvalidation\terror\tstops\tdecreasing and\tactually\tstarts\tto\tgo\tback\tup.\tThis\tindicates\tthat\tthe\tmodel\thas\tstarted\tto overfit\tthe\ttraining\tdata.\tWith\tearly\tstopping\tyou\tjust\tstop\ttraining\tas\tsoon\tas\tthe validation\terror\treaches\tthe\tminimum.\tIt\tis\tsuch\ta\tsimple\tand\tefficient regularization\ttechnique\tthat\tGeoffrey\tHinton\tcalled\tit\ta\t“beautiful\tfree\tlunch.”\n",
    "\n",
    "- TIP \n",
    "- With\tStochastic\tand\tMini-batch\tGradient\tDescent,\tthe\tcurves\tare\tnot\tso\tsmooth,\tand\tit\tmay\tbe\thard to\tknow\twhether\tyou\thave\treached\tthe\tminimum\tor\tnot.\tOne\tsolution\tis\tto\tstop\tonly\tafter\tthe validation\terror\thas\tbeen\tabove\tthe\tminimum\tfor\tsome\ttime\t(when\tyou\tare\tconfident\tthat\tthe\tmodel will\tnot\tdo\tany\tbetter),\tthen\troll\tback\tthe\tmodel\tparameters\tto\tthe\tpoint\twhere\tthe\tvalidation\terror was\tat\ta\tminimum.\n",
    "\n",
    "- CHECK code -> very important -> Note\tthat\twith\twarm_start=True,\twhen\tthe\tfit()\tmethod\tis\tcalled,\tit\tjust continues\ttraining\twhere\tit\tleft\toff\tinstead\tof\trestarting\tfrom\tscratch.\n",
    "\n",
    "\n",
    "## Logistic\tRegression (Logit Regression)\n",
    "\n",
    "- some\tregression\talgorithms\tcan\tbe\tused\tfor classification\tas\twell\t(and\tvice\tversa)\n",
    "- used\tto\testimate\tthe\tprobability\tthat\tan\tinstance belongs\tto\ta\tparticular\tclass\n",
    "- binary\tclassifier.\n",
    "\n",
    "### Estimating\tProbabilities \n",
    "\n",
    "- The\tlogistic\t—\talso\tcalled\tthe\tlogit,\tnoted\tσ(·)\t—\tis\ta\tsigmoid\tfunction\t(i.e.,\tSshaped)\tthat\toutputs\ta\tnumber\tbetween\t0\tand\t1\n",
    "\n",
    "### Training\tand\tCost\tFunction \n",
    "\n",
    "- there\tis\tno\tknown\tclosed-form\tequation\tto\tcompute\tthe value\tof\tθ\tthat\tminimizes\tthis\tcost\tfunction\t(there\tis\tno\tequivalent\tof\tthe\tNormal Equation).\tBut\tthe\tgood\tnews\tis\tthat\tthis\tcost\tfunction(log loss)\tis\tconvex,\tso\tGradient Descent\t(or\tany\tother\toptimization\talgorithm)\tis\tguaranteed\tto\tfind\tthe\tglobal minimum\n",
    "\n",
    "### Decision\tBoundaries \n",
    "\n",
    "- Logistic\tRegression\tmodels\tcan\tbe\tregularized using\tℓ1\tor\tℓ2\tpenalties.\tScitkit-Learn\tactually\tadds\tan\tℓ2\tpenalty\tby\tdefault.\n",
    "\n",
    "- NOTE \n",
    "- The\thyperparameter\tcontrolling\tthe\tregularization\tstrength\tof\ta\tScikit-Learn\tLogisticRegression model\tis\tnot\talpha\t(as\tin\tother\tlinear\tmodels),\tbut\tits\tinverse:\tC.\tThe\thigher\tthe\tvalue\tof\tC,\tthe\tless the\tmodel\tis\tregularized.\n",
    "\n",
    "- CHECK code\n",
    "\n",
    "## Softmax\tRegression \n",
    "\n",
    "- The\tLogistic\tRegression\tmodel\tcan\tbe\tgeneralized\tto\tsupport\tmultiple\tclasses directly,\twithout\thaving\tto\ttrain\tand\tcombine\tmultiple\tbinary\tclassifiers\t(as discussed\tin\tChapter\t3).\tThis\tis\tcalled\tSoftmax\tRegression,\tor\tMultinomial Logistic\tRegression. \n",
    "\n",
    "- NOT COMPLETE READ AND FILL THIS PART\n",
    "\n",
    "\n",
    "# 5. SVM\n",
    "\n",
    "- capable of performing linear or nonlinear classification,regression, and even outlier detection.\n",
    "- particularly well suited for classification of complex butsmall- or medium-sized datasets.\n",
    "- SVMs are sensitive to the feature scales. Use Scikit-Learn’s StandardScaler for feature scaling.\n",
    "\n",
    "##  Linear SVM Classification\n",
    "\n",
    "- Large margin classification\n",
    "- support vectors\n",
    "\n",
    "### Soft Margin Classification\n",
    "        \n",
    "- Scikit-Learn’s SVM classes,a smaller C value leads to a wider street but more margin violations.\n",
    "- TIP -> If your SVM model is overfitting, you can try regularizing it by reducing C.\n",
    "- NOTE -> Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "- CHECK -> handson-ml-master/05_support_vector_machines notebook for Sklearn implementation(LinearSVC)\n",
    "        -- alternative \n",
    "            -> SVC(kernel=\"linear\", C=1),but it is much slower, especially with large training sets, so it is not recommended\n",
    "            -> SGDClassifier(loss=\"hinge\", alpha=1/(m*C)), \n",
    "                --> does not converge as fast as the LinearSVC class.\n",
    "                --> but it can be useful to handle huge datasets that do not fit in memory (out-of-core training)\n",
    "                --> or to handle online classification tasks.\n",
    "                \n",
    "- TIP \n",
    "The LinearSVC class regularizes the bias term, so you should center the training set first by\n",
    "subtracting its mean. This is automatic if you scale the data using the StandardScaler. Moreover,make sure you set the **loss hyperparameter to \"hinge\"**, as it is not the default value. Finally, forbetter performance you should set the **dual hyperparameter to False**, unless there are more featuresthan training instances.\n",
    "\n",
    "## Nonlinear SVM Classification\n",
    "\n",
    "- One approach to handling nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset\n",
    "\n",
    "    - Polynomial Kernel\n",
    "    -- CHECK -> handson-ml-master/05_support_vector_machines notebook for Sklearn implementation SVC(kernel=\"poly\", degree=3, coef0=1, C=5)\n",
    "    -- if your model is overfitting,you might want to reduce the polynomial degree\n",
    "    -- The hyperparameter coef0 controls how much themodel is influenced by high-degree polynomials versus low-degree polynomials.\n",
    "    \n",
    "- TIP\n",
    "\n",
    "A common approach to find the right hyperparameter values is to use grid search (see Chapter 2). Itis often faster to first do a very coarse grid search, then a finer grid search around the best valuesfound. Having a good sense of what each hyperparameter actually does can also help you search inthe right part of the hyperparameter space.\n",
    "\n",
    "\n",
    "- Another technique to tackle nonlinear problems is to add features computedusing a similarity function that measures how much each instance resembles aparticular landmark.\n",
    "    - Adding Similarity Features\n",
    "        - similarity function to be the GaussianRadial Basis Function (RBF) with γ = 0.3\n",
    "            - Gaussian RBF Kernel\n",
    "                - CHECK- sklearn implementation SVC(kernel=\"rbf\", gamma=5, C=0.001)\n",
    "                \n",
    "                - **Increasing** **gamma** makes the bell-shape curve narrower, and as a result each instance’s range of **influence is smaller**: the decision boundary ends up being **more irregular, wiggling around individual instances**. Conversely, a **small gamma value** makes the bell-shaped curve wider,so instances have a **larger range of influence**, and the decision boundary ends up **smoother**. So γ acts like a regularization hyperparameter: if your model is **overfitting**, you should **reduce** it, and if it is **underfitting**, you should **increase** it(similar to the C hyperparameter).\n",
    "\n",
    "-TIP\n",
    "\n",
    "With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, youshould always try the linear kernel first (remember that LinearSVC is much faster thanSVC(kernel=\"linear\")), especially if the training set is very large or if it has plenty of features. Ifthe training set is not too large, you should try the Gaussian RBF kernel as well; it works well inmost cases. Then if you have spare time and computing power, you can also experiment with a fewother kernels using cross-validation and grid search, especially if there are kernels specialized foryour training set’s data structure.\n",
    "\n",
    "\n",
    "- SVM Regression\n",
    "\n",
    "SVM Regression tries to fit as many instances as possible on the street whilelimiting margin violations (i.e., instances off the street). The width of the street iscontrolled by a hyperparameter ε\n",
    "    - ε-insensitive.\n",
    "    \n",
    "- CHECK -> sklearn implemention LinearSVR\n",
    "- alternative -> SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "\n",
    "- NOTE\n",
    "SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details.\n",
    "\n",
    "**Haven't learned under the hood section, read it when have time**\n",
    "\n",
    "\n",
    "\n",
    "# 6. Decision\tTrees\n",
    "\n",
    "- can perform\tboth\tclassification\tand\tregression\ttasks,\tand\teven\tmultioutput\ttasks. \n",
    "- capable\tof\tfitting\tcomplex\tdatasets\n",
    "- - CHECK-> for sklearn implemenation tree_reg\t=\tDecisionTreeClassifier(max_depth=2) \n",
    "\n",
    "- NOTE \n",
    "Scikit-Learn\tuses\tthe\tCART\talgorithm,\twhich\tproduces\tonly\tbinary\ttrees:\tnonleaf\tnodes\talways\thave two children\t(i.e.,\tquestions\tonly\thave\tyes/no\tanswers).\tHowever,\tother\talgorithms\tsuch\tas\tID3\tcan produce\tDecision\tTrees\twith\tnodes\tthat\thave\tmore\tthan\ttwo\tchildren.\n",
    "\n",
    "- can\talso\testimate\tthe\tprobability\tthat\tan\tinstance\tbelongs\tto\ta particular\tclass\tk\n",
    "- For\tsmall\ttraining\tsets\t(less\tthan\ta\tfew\tthousand\tinstances),\tScikitLearn\tcan\tspeed\tup\ttraining\tby presorting\tthe\tdata\t(set\tpresort=True),\tbut\tthis slows\tdown\ttraining\tconsiderably\tfor\tlarger\ttraining\tsets.\n",
    "\n",
    "- By\tdefault,\tthe\tGini\timpurity\tmeasure\tis\tused,\tbut\tyou\tcan\tselect\tthe\tentropy impurity\tmeasure\tinstead\tby\tsetting\tthe\t**criterion**\thyperparameter\tto **\"entropy\"**.\n",
    "- So\tshould\tyou\tuse\tGini\timpurity\tor\tentropy?The\ttruth\tis,\tmost\tof\tthe\ttime\tit does\tnot\tmake\ta\tbig\tdifference:\tthey\tlead\tto\tsimilar\ttrees\n",
    "\n",
    "## Regularization\tHyperparameters\n",
    "\n",
    "- Reducing\tmax_depth\twill\tregularize\tthe\tmodel\tand\tthus reduce\tthe\trisk\tof\toverfitting\n",
    "\n",
    "- The\tDecisionTreeClassifier\tclass\thas\ta\tfew\tother\tparameters\tthat\tsimilarly restrict\tthe\tshape\tof\tthe\tDecision\tTree:\tmin_samples_split\t(the\tminimum number\tof\tsamples\ta\tnode\tmust\thave\tbefore\tit\tcan\tbe\tsplit),\tmin_samples_leaf (the\tminimum\tnumber\tof\tsamples\ta\tleaf\tnode\tmust\thave), min_weight_fraction_leaf\t(same\tas\tmin_samples_leaf\tbut\texpressed\tas\ta fraction\tof\tthe\ttotal\tnumber\tof\tweighted\tinstances),\tmax_leaf_nodes\t(maximum number\tof\tleaf\tnodes),\tand\tmax_features\t(maximum\tnumber\tof\tfeatures\tthat\tare evaluated\tfor\tsplitting\tat\teach\tnode).\tIncreasing\tmin_*\thyperparameters\tor reducing\tmax_*\thyperparameters\twill\tregularize\tthe\tmodel.\n",
    "\n",
    "- NOTE \n",
    "Other\talgorithms\twork\tby\tfirst\ttraining\tthe\tDecision\tTree\twithout\trestrictions,\tthen\tpruning\t(deleting) unnecessary\tnodes.\tA\tnode\twhose\tchildren\tare\tall\tleaf\tnodes\tis\tconsidered\tunnecessary\tif\tthe\tpurity improvement\tit\tprovides\tis\tnot\tstatistically\tsignificant.\tStandard\tstatistical\ttests,\tsuch\tas\tthe\tχ2\ttest, are\tused\tto\testimate\tthe\tprobability\tthat\tthe\timprovement\tis\tpurely\tthe\tresult\tof\tchance\t(which\tis called\tthe\tnull\thypothesis).\tIf\tthis\tprobability,\tcalled\tthe\tp-value,\tis\thigher\tthan\ta\tgiven\tthreshold (typically\t5%,\tcontrolled\tby\ta\thyperparameter),\tthen\tthe\tnode\tis\tconsidered\tunnecessary\tand\tits children\tare\tdeleted.\tThe\tpruning\tcontinues\tuntil\tall\tunnecessary\tnodes\thave\tbeen\tpruned.\n",
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "- CHECK-> for sklearn implemenation tree_reg\t=\tDecisionTreeRegressor(max_depth=2) \n",
    "\n",
    "## Instability\n",
    "\n",
    "- few\tlimitations\n",
    "    - sensitive\tto\ttraining\tset\trotation. \n",
    "        - One way to\tlimit\tthis\tproblem\tis\tto\tuse\tPCA, which\toften results in a\tbetter\torientation\tof\tthe\ttraining data.\n",
    "        - since\tthe\ttraining\talgorithm\tused by\tScikit-Learn\tis\tstochastic\tyou\tmay\tget\tvery\tdifferent\tmodels\teven\ton\tthe same\ttraining\tdata\t(unless\tyou\tset\tthe\trandom_state\thyperparameter).\n",
    "        - Random\tForests\tcan\tlimit\tthis\tinstability\tby\taveraging\tpredictions\tover\tmany trees\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Ensemble\tLearning\tand Random\tForests\n",
    "\n",
    "- you\twill\toften\tuse\tEnsemble\tmethods near\tthe\tend\tof\ta\tproject,\tonce\tyou\thave\talready\tbuilt\ta\tfew\tgood\tpredictors,\tto combine\tthem\tinto\tan\teven\tbetter\tpredictor.\tIn\tfact,\tthe\twinning\tsolutions\tin Machine\tLearning\tcompetitions\toften\tinvolve\tseveral\tEnsemble\tmethods\t(most famously\tin\tthe\tNetflix\tPrize\tcompetition). \n",
    "\n",
    "## Voting\tClassifiers \n",
    "\n",
    "- \thard\tvoting\tclassifier\n",
    "- TIP \n",
    "Ensemble\tmethods\twork\tbest\twhen\tthe\tpredictors\tare\tas\tindependent\tfrom\tone\tanother\tas\tpossible. One\tway\tto get\tdiverse\tclassifiers\tis\tto\ttrain\tthem\tusing\tvery\tdifferent\talgorithms.\t\n",
    "- CHECK -> Sklearn implementaion of voting_clf\t=\tVotingClassifier(estimators=[('lr',\tlog_clf),\t('rf',\trnd_clf),\t('svc',\tsvm_clf)],voting='hard') \n",
    "\n",
    "- If\tall\tclassifiers\tare\table\tto\testimate\tclass\tprobabilities\t(i.e.,\tthey\thave\ta predict_proba()\tmethod),\tthen\tyou\tcan\ttell\tScikit-Learn\tto\tpredict\tthe\tclass with\tthe\thighest\tclass\tprobability,\taveraged\tover\tall\tthe\tindividual\tclassifiers. This\tis\tcalled\tsoft\tvoting.\n",
    "\n",
    "- All\tyou\tneed\tto\tdo\tis replace\tvoting=\"hard\"\twith\tvoting=\"soft\"\tand\tensure\tthat\tall\tclassifiers\tcan estimate\tclass\tprobabilities.\n",
    "\n",
    "## Bagging\tand\tPasting \n",
    "\n",
    "- One\tway\tto\tget\ta\tdiverse\tset\tof\tclassifiers\tis\tto\tuse\tvery\tdifferent\ttraining algorithms,\tas\tjust\tdiscussed.\tAnother\tapproach\tis\tto\tuse\tthe\tsame\ttraining algorithm\tfor\tevery\tpredictor,\tbut\tto\ttrain\tthem\ton\tdifferent\trandom\tsubsets\tof the\ttraining\tset.\tWhen\tsampling\tis\tperformed\twith\treplacement,\tthis\tmethod\tis called\tbagging1\t(short\tfor\tbootstrap\taggregating2).\tWhen\tsampling\tis\tperformed without\treplacement,\tit\tis\tcalled\tpasting\n",
    "\n",
    "- \tpredictors\tcan\tall\tbe\ttrained\tin\tparallel,\tvia different\tCPU\tcores\tor\teven\tdifferent\tservers.\tSimilarly,\tpredictions\tcan\tbe\tmade in\tparallel.\tThis\tis\tone\tof\tthe\treasons\twhy\tbagging\tand\tpasting\tare\tsuch\tpopular methods:\tthey\tscale\tvery\twell.\n",
    "\n",
    "- Scikit-Learn\toffers\ta\tsimple\tAPI\tfor\tboth\tbagging\tand\tpasting\twith\tthe BaggingClassifier\tclass\t(or\tBaggingRegressor\tfor\tregression).\tThe\tfollowing code\ttrains\tan\tensemble\tof\t500\tDecision\tTree\tclassifiers,5\teach\ttrained\ton\t100 training\tinstances\trandomly\tsampled\tfrom\tthe\ttraining\tset\twith\treplacement\t(this is\tan\texample\tof\tbagging,\tbut\tif\tyou\twant\tto\tuse\tpasting\tinstead,\tjust\tset bootstrap=False).\tThe\tn_jobs\tparameter\ttells\tScikit-Learn\tthe\tnumber\tof\tCPU cores\tto\tuse\tfor\ttraining\tand\tpredictions\t(–1\ttells\tScikit-Learn\tto\tuse\tall\tavailable cores):\n",
    "\n",
    "- CHECK -> Sklearn implementation -> bag_clf\t=\tBaggingClassifier(\t\t\t\tDecisionTreeClassifier(),\tn_estimators=500,\t\t\t\tmax_samples=100,\tbootstrap=True,\tn_jobs=-1)\n",
    "\n",
    "- NOTE \n",
    "- The\tBaggingClassifier\tautomatically\tperforms\tsoft\tvoting\tinstead\tof\thard\tvoting\tif\tthe\tbase classifier\tcan\testimate\tclass\tprobabilities\t(i.e.,\tif\tit\thas\ta\tpredict_proba()\tmethod),\twhich\tis\tthe\tcase with\tDecision\tTrees\tclassifiers.\n",
    "\n",
    "## Out-of-Bag\tEvaluation\n",
    "\n",
    "- In\tScikit-Learn,\tyou\tcan\tset\toob_score=True\twhen\tcreating\ta BaggingClassifier\tto\trequest\tan\tautomatic\toob\tevaluation\tafter\ttraining\n",
    "\n",
    "\n",
    "## Random\tPatches\tand\tRandom\tSubspaces \n",
    "\n",
    "- The\tBaggingClassifier\tclass\tsupports\tsampling\tthe\tfeatures\tas\twell.\tThis\tis controlled\tby\ttwo\thyperparameters:\tmax_features\tand\tbootstrap_features. \n",
    "\n",
    "- This\tis\tparticularly\tuseful\twhen\tyou\tare\tdealing\twith\thigh-dimensional\tinputs (such\tas\timages).\tSampling\tboth\ttraining\tinstances\tand\tfeatures\tis\tcalled\tthe Random\tPatches\tmethod.7\tKeeping\tall\ttraining\tinstances\t(i.e.,\tbootstrap=False and\tmax_samples=1.0)\tbut\tsampling\tfeatures\t(i.e.,\tbootstrap_features=True and/or\tmax_features\tsmaller\tthan\t1.0)\tis\tcalled\tthe\tRandom\tSubspaces\tmethod.8 Sampling\tfeatures\tresults\tin\teven\tmore\tpredictor\tdiversity,\ttrading\ta\tbit\tmore bias\tfor\ta\tlower\tvariance.\n",
    "\n",
    "## Random\tForests \n",
    "\n",
    "- \tRandom\tForest9\tis\tan\tensemble\tof\tDecision\tTrees, generally\ttrained\tvia\tthe\tbagging\tmethod\t(or\tsometimes\tpasting),\ttypically\twith max_samples\tset\tto\tthe\tsize\tof\tthe\ttraining\tset.\t\n",
    "\n",
    "- CHECK -> Sklearn implementataion -> rnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tmax_leaf_nodes=16,\tn_jobs=-1) \n",
    "\n",
    "### Extra-Trees \n",
    "\n",
    "- It\tis possible\tto\tmake\ttrees\teven\tmore\trandom\tby\talso\tusing\trandom\tthresholds\tfor each\tfeature\trather\tthan\tsearching\tfor\tthe\tbest\tpossible\tthresholds\t(like\tregular Decision\tTrees\tdo). A\tforest\tof\tsuch\textremely\trandom\ttrees\tis\tsimply\tcalled\tan\tExtremely Randomized\tTrees\tensemble12\t(or\tExtra-Trees\tfor\tshort)\n",
    "\n",
    "- You\tcan\tcreate\tan\tExtra-Trees\tclassifier\tusing\tScikit-Learn’s ExtraTreesClassifier\tclass.\n",
    "\n",
    "-TIP\n",
    "    - It\tis\thard\tto\ttell\tin\tadvance\twhether\ta\tRandomForestClassifier\twill\tperform\tbetter\tor\tworse\tthan\tan ExtraTreesClassifier.\tGenerally,\tthe\tonly\tway\tto\tknow\tis\tto\ttry\tboth\tand\tcompare\tthem\tusing cross-validation\t(and\ttuning\tthe\thyperparameters\tusing\tgrid\tsearch).\n",
    "\n",
    "### Feature\tImportance\n",
    "\n",
    "- Random\tForests\tare\tvery\thandy\tto\tget\ta\tquick\tunderstanding\tof\twhat\tfeatures actually\tmatter,\tin\tparticular\tif\tyou\tneed\tto\tperform\tfeature\tselection.\n",
    "\n",
    "- CHECK ->\n",
    "\n",
    "\trnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tn_jobs=-1) >>>\trnd_clf.fit(iris[\"data\"],\tiris[\"target\"]) >>>\tfor\tname,\tscore\tin\tzip(iris[\"feature_names\"],\trnd_clf.feature_importances_): ...\t\t\t\t\tprint(name,\tscore) \n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "- Boosting\t(originally\tcalled\thypothesis\tboosting)\trefers\tto\tany\tEnsemble\tmethod that\tcan\tcombine\tseveral\tweak\tlearners\tinto\ta\tstrong\tlearner.\tThe\tgeneral\tidea\tof most\tboosting\tmethods\tis\tto\ttrain\tpredictors\tsequentially,\teach\ttrying\tto\tcorrect its\tpredecessor.There\tare\tmany\tboosting\tmethods\tavailable,\tbut\tby\tfar\tthe\tmost popular\tare\tAdaBoost13\t(short\tfor\tAdaptive\tBoosting)\tand\tGradient\tBoosting\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "- WARNING\n",
    "\n",
    "drawback\tto\tthis\tsequential\tlearning\ttechnique:\tit\tcannot\tbe\tparallelized\t(or only\tpartially),\tsince\teach\tpredictor\tcan\tonly\tbe\ttrained\tafter\tthe\tprevious\tpredictor\thas\tbeen\ttrained and\tevaluated.\tAs\ta\tresult,\tit\tdoes\tnot\tscale\tas\twell\tas\tbagging\tor\tpasting.\n",
    "\n",
    "- Scikit-Learn\tactually\tuses\ta\tmulticlass\tversion\tof\tAdaBoost\tcalled\tSAMME16 (which\tstands\tfor\tStagewise\tAdditive\tModeling\tusing\ta\tMulticlass\tExponential loss\tfunction).\tWhen\tthere\tare\tjust\ttwo\tclasses,\tSAMME\tis\tequivalent\tto AdaBoost\n",
    "\n",
    "- if\tthe\tpredictors\tcan\testimate\tclass\tprobabilities\t(i.e.,\tif they\thave\ta\tpredict_proba()\tmethod),\tScikit-Learn\tcan\tuse\ta\tvariant\tof SAMME\tcalled\tSAMME.R\t(the\tR\tstands\tfor\t“Real”),\twhich\trelies\ton\tclass probabilities\trather\tthan\tpredictions\tand\tgenerally\tperforms\tbetter. \n",
    "\n",
    "- CHECK -> Sklearn implementation -> AdaBoostClassifier(\t\t\t\tDecisionTreeClassifier(max_depth=1),\tn_estimators=200,\t\t\t\talgorithm=\"SAMME.R\",\tlearning_rate=0.5) \n",
    "\n",
    "- TIP \n",
    "- If\tyour\tAdaBoost\tensemble\tis\toverfitting\tthe\ttraining\tset,\tyou\tcan\ttry\treducing\tthe\tnumber\tof estimators\tor\tmore\tstrongly\tregularizing\tthe\tbase\testimator.\n",
    "\n",
    "\n",
    "## Gradient\tBoosting\n",
    "\n",
    "- Just\tlike AdaBoost,\tGradient\tBoosting\tworks\tby\tsequentially\tadding\tpredictors\tto\tan ensemble,\teach\tone\tcorrecting\tits\tpredecessor.\tHowever,\tinstead\tof\ttweaking\tthe instance\tweights\tat\tevery\titeration\tlike\tAdaBoost\tdoes,\tthis\tmethod\ttries\tto\tfit the\tnew\tpredictor\tto\tthe\tresidual\terrors\tmade\tby\tthe\tprevious\tpredictor. \n",
    "\n",
    "- CHECK ->  sklearn implementation -> gbrt\t=\tGradientBoostingRegressor(max_depth=2,\tn_estimators=3,\tlearning_rate=1.0) \n",
    "\n",
    "- The\tlearning_rate\thyperparameter\tscales\tthe\tcontribution\tof\teach\ttree.\tIf\tyou set\tit\tto\ta\tlow\tvalue,\tsuch\tas\t0.1,\tyou\twill\tneed\tmore\ttrees\tin\tthe\tensemble\tto\tfit the\ttraining\tset,\tbut\tthe\tpredictions\twill\tusually\tgeneralize\tbetter.\t\n",
    "\n",
    "- In\torder\tto\tfind\tthe\toptimal\tnumber\tof\ttrees,\tyou\tcan\tuse\tearly\tstopping\t(see Chapter\t4).\tA\tsimple\tway\tto\timplement\tthis\tis\tto\tuse\tthe\tstaged_predict() method\n",
    "\n",
    "- CHECK ->  sklearn implementation\n",
    "\n",
    "- (instead\tof\ttraining\ta\tlarge\tnumber\tof\ttrees\tfirst\tand\tthen\tlooking\tback\tto\tfind\tthe optimal\tnumber).\tYou\tcan\tdo\tso\tby\tsetting\twarm_start=True,\twhich\tmakes Scikit-Learn\tkeep\texisting\ttrees\twhen\tthe\tfit()\tmethod\tis\tcalled,\tallowing incremental\ttraining.\tThe\tfollowing\tcode\tstops\ttraining\twhen\tthe\tvalidation\terror does\tnot\timprove\tfor\tfive\titerations\tin\ta\trow:\n",
    "\n",
    "- The\tGradientBoostingRegressor\tclass\talso\tsupports\ta\tsubsample hyperparameter,\twhich\tspecifies\tthe\tfraction\tof\ttraining\tinstances\tto\tbe\tused\tfor training\teach\ttree.\tFor\texample,\tif\tsubsample=0.25,\tthen\teach\ttree\tis\ttrained\ton 25%\tof\tthe\ttraining\tinstances,\tselected\trandomly.\tThis\ttechnique\tis\tcalled\tStochastic\tGradient\tBoosting.\n",
    "\n",
    "\n",
    "## Stacking\n",
    "\n",
    "- Unfortunately,\tScikit-Learn\tdoes\tnot\tsupport\tstacking\tdirectly,\tbut\tit\tis\tnot\ttoo hard\tto\troll\tout\tyour\town\timplementation\t(see\tthe\tfollowing\texercises). Alternatively,\tyou\tcan\tuse\tan\topen\tsource\timplementation\tsuch\tas\tbrew (available\tat\thttps://github.com/viisar/brew).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. Dimensionality\tReduction\n",
    "\n",
    "- \tin\treal-world\tproblems,\tit\tis\toften\tpossible\tto\treduce\tthe\tnumber\tof features\tconsiderably,\tturning\tan\tintractable\tproblem\tinto\ta\ttractable\tone.\n",
    "\n",
    "- WARNING \n",
    "- Reducing\tdimensionality\tdoes\tlose\tsome\tinformation\t(just\tlike\tcompressing\tan\timage\tto\tJPEG\tcan degrade\tits\tquality),\tso\teven\tthough\tit\twill\tspeed\tup\ttraining,\tit\tmay\talso\tmake\tyour\tsystem\tperform slightly\tworse.\tIt\talso\tmakes\tyour\tpipelines\ta\tbit\tmore\tcomplex\tand\tthus\tharder\tto\tmaintain.\tSo\tyou should\tfirst\ttry\tto\ttrain\tyour\tsystem\twith\tthe\toriginal\tdata\tbefore\tconsidering\tusing\tdimensionality reduction\tif\ttraining\tis\ttoo\tslow.\tIn\tsome\tcases,\thowever,\treducing\tthe\tdimensionality\tof\tthe\ttraining data\tmay\tfilter\tout\tsome\tnoise\tand\tunnecessary\tdetails\tand\tthus\tresult\tin\thigher\tperformance\t(but\tin general\tit\twon’t;\tit\twill\tjust\tspeed\tup\ttraining).\n",
    "\n",
    "- Apart\tfrom\tspeeding\tup\ttraining,\tdimensionality\treduction\tis\talso\textremely useful\tfor\tdata\tvisualization\t(or\tDataViz).\n",
    "\n",
    "## The\tCurse\tof\tDimensionality \n",
    "\n",
    "- \tthe more\tdimensions\tthe\ttraining\tset\thas,\tthe\tgreater\tthe\trisk\tof\toverfitting\tit. \n",
    "\n",
    "## Main\tApproaches\tfor\tDimensionality\tReduction \n",
    "\n",
    "- \ttwo\tmain\tapproaches\tto\treducing\tdimensionality:\tprojection\tand Manifold\tLearning.\n",
    "\n",
    "### Projection\n",
    "\n",
    "### Manifold\tLearning \n",
    "\n",
    "## PCA\n",
    "\n",
    "- it\tidentifies\tthe\thyperplane\tthat\tlies\tclosest\tto\tthe\tdata, and\tthen\tit\tprojects\tthe\tdata\tonto\tit.\n",
    "\n",
    "### Preserving\tthe\tVariance \n",
    "\n",
    "- \tyou first\tneed\tto\tchoose\tthe\tright\thyperplane.\t\n",
    "-  select\tthe\taxis\tthat\tpreserves\tthe\tmaximum\tamount\tof variance\n",
    "\n",
    "### Principal\tComponents\n",
    "\n",
    "### Projecting\tDown\tto\td\tDimensions \n",
    "\n",
    "### Using\tScikit-Learn \n",
    "\n",
    "- CHECK -> for sklearn implementation PCA(n_components\t=\t2) \n",
    "\n",
    "- you\tcan\taccess\tthe\tprincipal components\tusing\tthe\tcomponents_\tvariable\t(note\tthat\tit\tcontains\tthe\tPCs\tas horizontal\tvectors,\tso,\tfor\texample,\tthe\tfirst\tprincipal\tcomponent\tis\tequal\tto pca.components_.T[:,\t0]).\n",
    "\n",
    "### Explained\tVariance\tRatio \n",
    "\n",
    "- explained_variance_ratio_\tvariable.\tIt indicates\tthe\tproportion\tof\tthe\tdataset’s\tvariance\tthat\tlies\talong\tthe\taxis\tof\teach principal\tcomponent.\n",
    "\n",
    "### Choosing\tthe\tRight\tNumber\tof\tDimensions\n",
    "\n",
    "- Instead\tof\tarbitrarily\tchoosing\tthe\tnumber\tof\tdimensions\tto\treduce\tdown\tto,\tit\tis generally\tpreferable\tto\tchoose\tthe\tnumber\tof\tdimensions\tthat\tadd\tup\tto\ta sufficiently\tlarge\tportion\tof\tthe\tvariance\t(e.g.,\t95%).\tUnless,\tof\tcourse,\tyou\tare reducing\tdimensionality\tfor\tdata\tvisualization\t—\tin\tthat\tcase\tyou\twill\tgenerally want\tto\treduce\tthe\tdimensionality\tdown\tto\t2\tor\t3.\n",
    "\n",
    "- \tyou\tcan\tset\tn_components\tto\tbe\ta\tfloat\tbetween\t0.0\tand 1.0,\tindicating\tthe\tratio\tof\tvariance\tyou\twish\tto\tpreserve:\n",
    "pca\t=\tPCA(n_components=0.95)\n",
    "\n",
    "- CHECK -> sklearn implementation\n",
    "\n",
    "- Yet\tanother\toption\tis\tto\tplot\tthe\texplained\tvariance\tas\ta\tfunction\tof\tthe\tnumber of\tdimensions\t(simply\tplot\tcumsum;\tsee\tFigure\t8-8).\tThere\twill\tusually\tbe\tan elbow\tin\tthe\tcurve,\twhere\tthe\texplained\tvariance\tstops\tgrowing\tfast.\tYou\tcan think\tof\tthis\tas\tthe\tintrinsic\tdimensionality\tof\tthe\tdataset.\tIn\tthis\tcase,\tyou\tcan see\tthat\treducing\tthe\tdimensionality\tdown\tto\tabout\t100\tdimensions\twouldn’t lose\ttoo\tmuch\texplained\tvariance.\n",
    "\n",
    "### PCA\tfor\tCompression\n",
    "\n",
    "- uses\tthe\tinverse_transform()\tmethod\tto decompress\n",
    "- CHECK -> sklearn implementation\n",
    "\n",
    "### Incremental\tPCA\n",
    "\n",
    "- \tIncremental\tPCA\t(IPCA)\talgorithms\thave\tbeen\tdeveloped:\tyou\tcan split\tthe\ttraining\tset\tinto\tmini-batches\tand\tfeed\tan\tIPCA\talgorithm\tone\tminibatch\tat\ta\ttime.\tThis\tis\tuseful\tfor\tlarge\ttraining\tsets,\tand\talso\tto\tapply\tPCA online\t(i.e.,\ton\tthe\tfly,\tas\tnew\tinstances\tarrive). \n",
    "\n",
    "- CHECK -> sklearn implementation\n",
    "\n",
    "### Randomized\tPCA \n",
    "\n",
    "- dramatically\tfaster\tthan\tthe\tprevious algorithms\twhen\td\tis\tmuch\tsmaller\tthan\tn.\n",
    "- CHECK -> sklearn implementation\n",
    "\n",
    "\n",
    "## Kernel\tPCA \n",
    "\n",
    "- \tIt\tis\toften\tgood\tat\tpreserving\tclusters\tof\tinstances after\tprojection,\tor\tsometimes\teven\tunrolling\tdatasets\tthat\tlie\tclose\tto\ta\ttwisted manifold. \n",
    "\n",
    "- CHECK -> sklearn implementation - rbf_pca\t=\tKernelPCA(n_components\t=\t2,\tkernel=\"rbf\",\tgamma=0.04) \n",
    "\n",
    "### Selecting\ta\tKernel\tand\tTuning\tHyperparameters \n",
    "\n",
    "- As\tkPCA\tis\tan\tunsupervised\tlearning\talgorithm,\tthere\tis\tno\tobvious\tperformance measure\tto\thelp\tyou\tselect\tthe\tbest\tkernel\tand\thyperparameter\tvalues.\tHowever, dimensionality\treduction\tis\toften\ta\tpreparation\tstep\tfor\ta\tsupervised\tlearning task\t(e.g.,\tclassification),\tso\tyou\tcan\tsimply\tuse\tgrid\tsearch\tto\tselect\tthe\tkernel and\thyperparameters\tthat\tlead\tto\tthe\tbest\tperformance\ton\tthat\ttask.\t\n",
    "\n",
    "\n",
    "## LLE\n",
    "\n",
    "- Locally\tLinear\tEmbedding\t(LLE)8\tis\tanother\tvery\tpowerful\tnonlinear dimensionality\treduction\t(NLDR)\ttechnique.\tIt\tis\ta\tManifold\tLearning\ttechnique that\tdoes\tnot\trely\ton\tprojections.\n",
    "\n",
    "- This\tmakes\tit\tparticularly\tgood\tat\tunrolling\ttwisted\tmanifolds, especially\twhen\tthere\tis\tnot\ttoo\tmuch\tnoise. \n",
    "\n",
    "- CHECK -> sklearn implementation\n",
    "\n",
    "## Other\tDimensionality\tReduction\t\n",
    "\n",
    "Techniques There\tare\tmany\tother\tdimensionality\treduction\ttechniques,\tseveral\tof\twhich\tare available\tin\tScikit-Learn.\tHere\tare\tsome\tof\tthe\tmost\tpopular:\n",
    "\t\n",
    "Multidimensional\tScaling\t(MDS)\treduces\tdimensionality\twhile\ttrying\tto preserve\tthe\tdistances\tbetween\tthe\tinstances\t(see\tFigure\t8-13). Isomap\tcreates\ta\tgraph\tby\tconnecting\teach\tinstance\tto\tits\tnearest\tneighbors, then\treduces\tdimensionality\twhile\ttrying\tto\tpreserve\tthe\tgeodesic distances9\tbetween\tthe\tinstances. t-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE)\treduces dimensionality\twhile\ttrying\tto\tkeep\tsimilar\tinstances\tclose\tand\tdissimilar instances\tapart.\tIt\tis\tmostly\tused\tfor\tvisualization,\tin\tparticular\tto\tvisualize clusters\tof\tinstances\tin\thigh-dimensional\tspace\t(e.g.,\tto\tvisualize\tthe MNIST\timages\tin\t2D). Linear\tDiscriminant\tAnalysis\t(LDA)\tis\tactually\ta\tclassification\talgorithm, but\tduring\ttraining\tit\tlearns\tthe\tmost\tdiscriminative\taxes\tbetween\tthe classes,\tand\tthese\taxes\tcan\tthen\tbe\tused\tto\tdefine\ta\thyperplane\tonto\twhich to\tproject\tthe\tdata.\tThe\tbenefit\tis\tthat\tthe\tprojection\twill\tkeep\tclasses\tas\tfar apart\tas\tpossible,\tso\tLDA\tis\ta\tgood\ttechnique\tto\treduce\tdimensionality before\trunning\tanother\tclassification\talgorithm\tsuch\tas\tan\tSVM\tclassifier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2\n",
    "\n",
    "# Neural\tNetworks\tand\tDeep Learning\n",
    "\n",
    "# 9. Up\tand\tRunning\twith TensorFlow\n",
    "\n",
    "- you\tfirst\tdefine\tin\tPython\ta\tgraph\tof computations\tto\tperform, and\tthen TensorFlow\ttakes\tthat\tgraph\tand\truns\tit\tefficiently\tusing\toptimized\tC++\tcode.\n",
    "\n",
    "- \tit\tis\tpossible\tto\tbreak\tup\tthe\tgraph\tinto\tseveral\tchunks\tand\trun them\tin\tparallel\tacross\tmultiple\tCPUs\tor\tGPUs\n",
    "\n",
    "- TensorFlow\talso\tsupports\tdistributed\tcomputing,\t\n",
    "\n",
    "## Installation\n",
    "\n",
    "## Creating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession \n",
    "\n",
    "- CHECK -> sklearn implementation -> Very important\n",
    "\n",
    "- A\tTensorFlow\tprogram\tis\ttypically\tsplit\tinto\ttwo\tparts:\tthe\tfirst\tpart\tbuilds\ta computation\tgraph\t(this\tis\tcalled\tthe\tconstruction\tphase),\tand\tthe\tsecond\tpart runs\tit\t(this\tis\tthe\texecution\tphase).\tThe\tconstruction\tphase\ttypically\tbuilds\ta computation\tgraph\trepresenting\tthe\tML\tmodel\tand\tthe\tcomputations\trequired\tto train\tit.\tThe\texecution\tphase\tgenerally\truns\ta\tloop\tthat\tevaluates\ta\ttraining\tstep repeatedly\t(for\texample,\tone\tstep\tper\tmini-batch),\tgradually\timproving\tthe model\tparameters.\tWe\twill\tgo\tthrough\tan\texample\tshortly.\n",
    "\n",
    "## Managing\tGraphs \n",
    "\n",
    "## Lifecycle\tof\ta\tNode\tValue\n",
    "\n",
    "## Linear\tRegression\twith\tTensorFlow \n",
    "\n",
    "-> CHECK -> tensorflow implementation\n",
    "\n",
    "## Implementing\tGradient\tDescent \n",
    "\n",
    "- When\tusing\tGradient\tDescent,\tremember\tthat\tit\tis\timportant\tto\tfirst\tnormalize\tthe\tinput\tfeature vectors,\tor\telse\ttraining\tmay\tbe\tmuch\tslower.\tYou\tcan\tdo\tthis\tusing\tTensorFlow,\tNumPy,\tScikitLearn’s\tStandardScaler,\tor\tany\tother\tsolution\tyou\tprefer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
